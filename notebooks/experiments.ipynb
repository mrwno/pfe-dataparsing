{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Comparison of Standardization Methods on GLUE Datasets\n\nEvaluate and compare four methods side-by-side:\n- **Keyword baseline** — rule-based synonym matching\n- **Embedding baseline** — cosine similarity via `all-MiniLM-L6-v2`\n- **Local LLM** — `Qwen/Qwen3-0.6B` running locally\n- **API LLM** — `claude-opus-4.6` via OpenRouter\n\nMetric: **Jaccard similarity** between predicted fields and Unitxt ground-truth fields."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Imports & Setup"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys, os\nsys.path.insert(0, os.path.abspath(\"..\"))  # notebooks/ → project root\n\nimport json\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport logging\nlogging.getLogger(\"unitxt\").setLevel(logging.ERROR)\n\nfrom src.eval import evaluate\nfrom src.baselines import baseline_keyword_match, baseline_embedding_match\nfrom src.standardize_api import load_standardized_dataset\nfrom src.standardize_local import load_standardized_dataset_local\n\nprint(\"Imports OK\")"
  },
  {
   "cell_type": "code",
   "source": "import os\nos.environ[\"OPENROUTER_API_KEY\"] = \"your_key_here\"  # replace before running",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 2. Datasets & Methods"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "GLUE_DATASETS = [\n    {\"card_id\": \"sst2\", \"hf_name\": \"glue\", \"hf_config\": \"sst2\"},\n    {\"card_id\": \"mrpc\", \"hf_name\": \"glue\", \"hf_config\": \"mrpc\"},\n    {\"card_id\": \"qnli\", \"hf_name\": \"glue\", \"hf_config\": \"qnli\"},\n    {\"card_id\": \"mnli\", \"hf_name\": \"glue\", \"hf_config\": \"mnli\"},\n    {\"card_id\": \"wnli\", \"hf_name\": \"glue\", \"hf_config\": \"wnli\"},\n]\n\nMETHODS = {\n    \"keyword\":   baseline_keyword_match,\n    \"embedding\": baseline_embedding_match,\n    \"local_llm\": load_standardized_dataset_local,\n    \"api_llm\":   load_standardized_dataset,\n}\n\nprint(f\"{len(GLUE_DATASETS)} datasets  ×  {len(METHODS)} methods  =  {len(GLUE_DATASETS) * len(METHODS)} evaluations\")"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 3. Run Evaluation\n\nEach cell below runs one method group. Run them independently to avoid re-running the expensive LLM calls."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "raw_results = []  # [{dataset, method, score, struct_score, annot_score}, ...]\n\nfor exp in GLUE_DATASETS:\n    card_id, hf_name, hf_config = exp[\"card_id\"], exp[\"hf_name\"], exp[\"hf_config\"]\n    print(f\"\\n── {card_id} ──\")\n\n    for method_name, standardize_fn in METHODS.items():\n        print(f\"  {method_name}...\", end=\" \", flush=True)\n        try:\n            result = evaluate(\n                hf_name=hf_name,\n                hf_config=hf_config,\n                card_id=card_id,\n                standardize_fn=standardize_fn,\n            )\n            score        = result[\"score\"]\n            struct_score = result[\"struct_score\"]\n            annot_score  = result[\"annot_score\"]\n            print(f\"✓  score={score:.3f}  struct={struct_score:.3f}  annot={annot_score:.3f}\")\n        except Exception as e:\n            score = struct_score = annot_score = None\n            print(f\"✗  {e}\")\n\n        raw_results.append({\n            \"dataset\":      card_id,\n            \"method\":       method_name,\n            \"score\":        score,\n            \"struct_score\": struct_score,\n            \"annot_score\":  annot_score,\n        })\n\nprint(\"\\n✅ Evaluation complete.\")"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 4. Results Table"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "COL_ORDER = [\"keyword\", \"embedding\", \"local_llm\", \"api_llm\"]\n\ndf_results = pd.DataFrame(raw_results)\n\ndef make_pivot(score_col: str) -> pd.DataFrame:\n    p = (\n        df_results\n        .pivot(index=\"dataset\", columns=\"method\", values=score_col)\n        .reindex(columns=[c for c in COL_ORDER if c in df_results[\"method\"].values])\n    )\n    p.loc[\"Average\"] = p.mean()\n    return p.round(3)\n\n# ── Combined score (struct + annot) / 2 ──────────────────\nprint(\"=== Combined score  (struct + annot) / 2 ===\")\ndisplay(make_pivot(\"score\"))\n\n# ── Structural score: Jaccard on field names ──────────────\nprint(\"\\n=== Structural score  (Jaccard on field names) ===\")\ndisplay(make_pivot(\"struct_score\"))\n\n# ── Annotation score: value match for *_type, type_of_*, classes ──\nprint(\"\\n=== Annotation score  (value match for *_type / type_of_* / classes) ===\")\ndisplay(make_pivot(\"annot_score\"))",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Visualization"
  },
  {
   "cell_type": "code",
   "source": "pivot_data = pivot.drop(index=\"Average\").astype(float)\navg = pivot.loc[\"Average\"].astype(float)\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 4))\n\n# ── Heatmap ──────────────────────────────────────────────\nsns.heatmap(\n    pivot_data,\n    ax=axes[0],\n    annot=True, fmt=\".2f\",\n    cmap=\"YlGn\", vmin=0, vmax=1,\n    linewidths=0.5, linecolor=\"white\",\n)\naxes[0].set_title(\"Jaccard score per dataset & method\", fontsize=12)\naxes[0].set_xlabel(\"\")\naxes[0].set_ylabel(\"\")\naxes[0].tick_params(axis=\"x\", rotation=30)\n\n# ── Average bar chart ─────────────────────────────────────\ncolors = sns.color_palette(\"Set2\", len(avg))\nbars = axes[1].bar(avg.index, avg.values, color=colors, edgecolor=\"black\", width=0.5)\naxes[1].set_title(\"Average Jaccard score per method\", fontsize=12)\naxes[1].set_ylim(0, 1.1)\naxes[1].set_ylabel(\"Jaccard score\")\naxes[1].tick_params(axis=\"x\", rotation=30)\nfor bar in bars:\n    axes[1].text(\n        bar.get_x() + bar.get_width() / 2,\n        bar.get_height() + 0.02,\n        f\"{bar.get_height():.3f}\",\n        ha=\"center\", va=\"bottom\", fontsize=10,\n    )\n\nplt.tight_layout()\nplt.savefig(\"../results/comparison_plot.png\", dpi=150, bbox_inches=\"tight\")\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pfe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}